'\" t
.\"___INFO__MARK_BEGIN__
.\"
.\" Copyright: 2004 by Sun Microsystems, Inc.
.\"
.\" Portions of this software are Copyright (c) 2014 Univa Corporation
.\"
.\"___INFO__MARK_END__
.\"
.\" Some handy macro definitions [from Tom Christensen's man(1) manual page].
.\"
.de SB		\" small and bold
.if !"\\$1"" \\s-2\\fB\&\\$1\\s0\\fR\\$2 \\$3 \\$4 \\$5
..
.\"
.de T		\" switch to typewriter font
.ft CW		\" probably want CW if you don't have TA font
..
.\"
.de TY		\" put $1 in typewriter font
.if t .T
.if n ``\c
\\$1\c
.if t .ft P
.if n \&''\c
\\$2
..
.\"
.de M		\" man page reference
\\fI\\$1\\fR\\|(\\$2)\\$3
..
.TH SCHED_CONF 5 "UGE 8.4.4" "Univa Grid Engine File Formats"
.\"
.SH NAME
sched_conf \- Univa Grid Engine default scheduler configuration file
.\"
.\"
.SH DESCRIPTION
.I sched_conf
defines the configuration file format for Univa Grid Engine's  scheduler. 
In order to modify the configuration, 
use the graphical user's interface
.M qmon 1
or the
.I -msconf
option of the 
.M qconf 1
command. A default configuration is provided together with the 
Univa Grid Engine distribution package.
.PP
Note, Univa Grid Engine allows backslashes (\\) be used to escape newline
(\\newline) characters. The backslash and the newline are replaced with a
space (" ") character before any interpretation.
.\"
.\"
.SH FORMAT
The following parameters are recognized by the Univa Grid Engine scheduler if
present in \fIsched_conf\fP:
.SS "\fBalgorithm\fP"
.B Note:
Deprecated, may be removed in future release.
.br
Allows for the selection of alternative scheduling algorithms.
.PP
Currently
.B default
is the only allowed setting.
.\"
.SS "\fBload_formula\fP"
A simple algebraic expression used to derive a single weighted
load value from all or part of the load parameters reported by
.M sge_execd 8
for each host and from all or part of the consumable resources (see
.M complex 5 )
being maintained for each host.
The load formula expression syntax is that of
a summation weighted load values, that is:
.sp 1
.nf
.RS
{w1|load_val1[*w1]}[{+|-}{w2|load_val2[*w2]}[{+|-}...]]
.RE
.fi
.sp 1
\fBNote\fP, no blanks are allowed in the load formula.
.br
The load values and consumable resources (load_val1, ...)
are specified by the name defined in the complex (see
.M complex 5 ).
.br
.B Note:
Administrator defined load values (see the
.B load_sensor
parameter in
.M sge_conf 5
for details)
and consumable resources available for all hosts (see
.M complex 5 )
may be used as well as Univa Grid Engine default load parameters.
.br
The weighting factors (w1, ...) are positive integers. After the expression
is evaluated for each host the results are assigned to the hosts and
are used to sort the hosts corresponding to the weighted load. The sorted
host list is used to sort queues subsequently.
.br
The default load formula is "np_load_avg".
.SS "\fBjob_load_adjustments\fP"
The load, which is imposed by the Univa Grid Engine jobs 
running on a system varies in time, and often, e.g. for the CPU load, 
requires some amount of time to be reported in the appropriate 
quantity by the operating system. Consequently, if a job was started 
very recently, the reported load may not provide a sufficient 
representation of the load which is already imposed on that host by 
the job. The reported load will adapt to the real load over time, but 
the period of time, in which the reported load is too low, may 
already lead to an oversubscription of that host. Univa Grid Engine allows 
the administrator to specify \fBjob_load_adjustments\fP which are used 
in the Univa Grid Engine scheduler to compensate for this problem.
.br
The \fBjob_load_adjustments\fP are specified as a comma separated list
of arbitrary load parameters or consumable resources and (separated by an
equal sign) an
associated load correction value. Whenever a job is dispatched to a
host by the scheduler,
the load parameter and consumable value set of that host
is increased by the values
provided in the \fBjob_load_adjustments\fP list. These correction
values are decayed linearly over time until after 
\fBload_adjustment_decay_time\fP from the start the corrections
reach the value 0.
If the \fBjob_load_adjustments\fP
list is assigned the special denominator NONE, no load corrections are
performed.
.br
The adjusted load and consumable values are used to compute the
combined and weighted
load of the hosts with the \fBload_formula\fP (see above) and to compare
the load and consumable values against the load threshold lists
defined in the queue configurations (see
.M queue_conf 5 ).
If the \fBload_formula\fP consists simply of the default CPU load average 
parameter \fInp_load_avg\fP, and if the jobs are very compute intensive, one might
want to set the \fBjob_load_adjustments\fP list to \fInp_load_avg=1.00\fP,
which means that every new job dispatched to a host will require
100 % CPU time, and thus the machine's load is instantly increased by 1.00.
.SS "\fBload_adjustment_decay_time\fP"
The load corrections in the "\fBjob_load_adjustments\fP" list above
are decayed linearly over time from the point of the job start, where
the corresponding load or consumable parameter is
raised by the full correction value,
until after a time period of "\fBload_adjustment_decay_time\fP", where the
correction becomes 0. Proper values for "\fBload_adjustment_decay_time\fP"
greatly depend upon the load or consumable parameters used and the
specific operating
system(s). Therefore, they can only be determined on-site and experimentally.
For the default \fInp_load_avg\fP load parameter a
"\fBload_adjustment_decay_time\fP" of 7 minutes has proven to yield reasonable
results.
.SS "\fBmaxujobs\fP"
The maximum number of jobs any user may have running in a Univa Grid Engine
cluster at the same time. If set to 0 (default) the users may run an arbitrary
number of jobs. 
.SS "\fBschedule_interval\fP"
At the time the scheduler thread initially registers at the event master thread in 
.M sge_qmaster 8 process
\fBschedule_interval\fP is used to set the time interval in which
the event master thread 
sends scheduling event updates to the scheduler thread.
A scheduling event is a status change that has occurred within
.M sge_qmaster 8
which may trigger or affect scheduler decisions (e.g. a job has
finished and thus the allocated resources are available again).
.br
In the Univa Grid Engine default scheduler the arrival of
a scheduling event report triggers a scheduler run. The scheduler
waits for event reports otherwise.
.br
\fBSchedule_interval\fP is a time value (see
.M queue_conf 5
for a definition of the syntax of time values).
.SS "\fBqueue_sort_method\fP"
This parameter determines in which order several criteria are taken into
account to product a sorted queue list. Currently, two settings are valid:
\fBseqno\fP and \fBload\fP. However in both cases, Univa Grid Engine attempts to
maximize the number of soft requests (see
.M qsub 1 
\fB\-s\fP option) being fulfilled by the queues for a particular as the
primary criterion.
.br
Then, if the \fBqueue_sort_method\fP parameter is set to \fBseqno\fP,
Univa Grid Engine will use the
.B seq_no
parameter as configured in the current queue configurations (see
.M queue_conf 5 )
as the next criterion to sort the queue list. The 
.B load_formula
(see above) has only a meaning if two queues have equal
sequence numbers.
If 
.B queue_sort_method
is set to \fBload\fP the load according the 
.B load_formula
is the criterion after maximizing a job's soft requests and the sequence
number is only used if two hosts have the same load.
The sequence number sorting is most 
useful if you want to define a fixed order in which queues are to be filled
(e.g.   the cheapest resource first).
.PP
The default for this parameter is \fBload\fP.
.\"
.SS "\fBhalftime\fP"
When executing under a share based policy, the scheduler "ages" (i.e. decreases)
usage to implement a sliding window for achieving the share entitlements
as defined by the share tree. The \fBhalftime\fP defines
the time interval in which accumulated usage will have been decayed
to half its original value. Valid values are specified in hours or according to 
the time format as specified in
.M queue_conf 5 .
.br
If the value is set to 0, the usage is not decayed.
.\"
.SS "\fBusage_weight_list\fP"
Univa Grid Engine accounts for the consumption of the resources wallclock-time, CPU-time, memory and IO
to determine the usage which is imposed on a system by a job. A single
usage value is computed from these four input parameters by multiplying
the individual values by weights and adding them up. The weights are
defined in the \fBusage_weight_list\fP. The format of the list is
.sp 1
.nf
.RS
wallclock=wwallclock,cpu=wcpu,mem=wmem,io=wio
.RE
.fi
.sp 1
where wwallclock, wcpu, wmem and wio are the configurable weights. The weights are real
numbers. The sum of all tree weights should be 1.
.\"
.SS "\fBcompensation_factor\fP"
Determines how fast Univa Grid Engine should compensate for past usage below of above
the share entitlement defined in the share tree. Recommended values are
between 2 and 10, where 10 means faster compensation.
.\"
.SS "\fBweight_user\fP"
The relative importance of the user shares in the functional policy.
Values are of type real.
.\"
.SS "\fBweight_project\fP"
The relative importance of the project shares in the functional policy.
Values are of type real.
.\"
.SS "\fBweight_department\fP"
The relative importance of the department shares in the
functional policy. Values are of type real.
.\"
.SS "\fBweight_job\fP"
The relative importance of the job shares in the
functional policy. Values are of type real.
.\"
.SS "\fBweight_tickets_functional\fP"
The maximum number of functional tickets available for distribution
by Univa Grid Engine. Determines the relative importance of the functional policy. 
See under 
.M sge_priority 5 
for an overview on job priorities.
.\"
.SS "\fBweight_tickets_share\fP"
The maximum number of share based tickets available for distribution
by Univa Grid Engine. Determines the relative importance of the share tree policy. See under 
.M sge_priority 5 
for an overview on job priorities.
.\"
.SS "\fBweight_deadline\fP"
The weight applied on the remaining time until a jobs latest start time. Determines 
the relative importance of the deadline. See under 
.M sge_priority 5 
for an overview on job priorities.
.\"
.SS "\fBweight_waiting_time\fP"
The weight applied on the jobs waiting time since submission. Determines 
the relative importance of the waiting time.
See under 
.M sge_priority 5 
for an overview on job priorities.
.\"
.SS "\fBweight_urgency\fP"
The weight applied on jobs normalized urgency when determining priority finally used.
Determines the relative importance of urgency.
See under 
.M sge_priority 5 
for an overview on job priorities.
.\"
.SS "\fBweight_priority\fP"
The weight applied on jobs normalized POSIX priority when determining priority
finally used. Determines the relative importance of POSIX priority.
See under
.M sge_priority 5
for an overview on job priorities.
.\"
.SS "\fBweight_ticket\fP"
The weight applied on normalized ticket amount when determining priority finally used.
Determines the relative importance of the ticket policies. See under 
.M sge_priority 5 
for an overview on job priorities.
.\"
.SS "\fBflush_finish_sec\fP"
The parameters are provided for tuning the system's scheduling behavior.
By default, a scheduler run is triggered in the scheduler interval. When
this parameter is set to 1 or larger, the scheduler will be triggered x seconds 
after a job has finished. Setting this parameter to 0 disables the flush after 
a job has finished.
.\"
.SS "\fBflush_submit_sec\fP"
The parameters are provided for tuning the system's scheduling behavior.
By default, a scheduler run is triggered in the scheduler interval.  When
this parameter is set to 1 or larger, the scheduler will be triggered  x seconds 
after a job was submitted to the system. Setting this parameter 
to 0 disables the flush after a job was submitted.
.\"
.SS "\fBschedd_job_info\fP"
The default scheduler can keep track why jobs could not be scheduled during
the last scheduler run. This parameter enables or disables the observation.
The value \fBtrue\fP enables the monitoring \fBfalse\fP turns it off.
.PP
It is also possible to activate the observation only for certain jobs. This
will be done if the parameter is set to \fBjob_list\fP followed by a comma 
separated list of job ids.
.PP
The user can obtain the collected information with the command qstat -j.
.\"
.SS "\fBparams\fP"
This is foreseen for passing additional parameters to the Univa Grid Engine
scheduler. The following values are recognized:
.\"
.IP "\fIDURATION_OFFSET\fP"
If set, overrides the default of value 60 seconds.  This parameter is used by 
the Univa Grid Engine scheduler when planning resource utilization as the delta
between net job runtimes and total time until resources become available 
again. Net job runtime as specified with -l h_rt=...  or -l s_rt=... or -l d_rt=... or
\fBdefault_duration\fP always differs from total job runtime due to delays before
and after actual job start and finish. Among the delays before job start is the time 
until the end of a \fBschedule_interval\fP, the time it takes to deliver a job to 
.M sge_execd 8
and the delays caused by \fBprolog\fP in
.M queue_conf 5
, \fBstart_proc_args\fP in
.M sge_pe 5
and \fBstarter_method\fP in
.M queue_conf 5 
. The delays after job finish include delays due to a forced job termination 
(\fBnotify\fP, \fBterminate_method\fP or \fBcheckpointing\fP), procedures run 
after actual job 
finish, such as \fBstop_proc_args\fP in
.M sge_pe 5 
or \fBepilog\fP in
.M queue_conf 5 
, and the delay until a new \fBschedule_interval\fP. 
.br
If the offset is too low, resource reservations (see \fBmax_reservation\fP)  
can be delayed repeatedly due to an overly optimistic job circulation time.
.\"
.IP "\fIJC_FILTER\fP"
.B Note:
Deprecated, may be removed in future release.
.br
If set to true, the scheduler limits the number of jobs it looks at during
a scheduling run. At the beginning of the scheduling run it assigns each
job a specific category, which is based on the job's requests, priority
settings, and the job owner. All scheduling policies will assign the same
importance to each job in one category. Therefore the number of jobs per
category have a FIFO order and can be limited to the number of free slots 
in the system.

A exception are jobs, which request a resource reservation. They are included 
regardless of the number of jobs in a category. 

This setting is turned off per default, because in very rare cases, the scheduler
can make a wrong decision. It is also advised to turn report_pjob_tickets off. 
Otherwise qstat -ext can report outdated ticket amounts. The information shown
with a qstat -j for a job, that was excluded in a scheduling run, is very limited.
.\"
.IP "\fIPROFILE\fP"
If set equal to 1, the scheduler logs profiling information summarizing
each scheduling run. In combination with \fIWARN_DISPATCHING_TIME\fP it is possible
to get profiling data for the longest and shortest job scheduling.
.\"
.IP "\fIMONITOR\fP"
If set equal to 1, the scheduler records information for each scheduling run allowing
to reproduce job resources utilization in the file \fI<sge_root>/<cell>/common/schedule\fP.\"
In order to see entries in the schedule file resource reservation must be turned
on (\fBmax_reservation\fP must be greater than 0) and jobs need a run-time
(using h_rt, s_rt, d_rt or setting a \fBdefault_duration\fP).
.br
The format of the schedule file is:
.br
<jobid>:         The jobs id.
.br
<taskid>:        The array task id or 1 in case of non-array jobs.
.br
<state>:         One of RUNNING, SUSPENDED, MIGRATING, STARTING, RESERVING.
.br
<start_time>:    Start time in seconds after 1.1.1970.
.br
<duration>:      Assumed job duration in seconds.
.br
<level_char>:    One of {P, G, H, Q} standing for {PE, Global, Host, Queue}.
.br
<object_name>:   The name of the PE, global, host, queue.
.br
<resource_name>: The name of the consumable resource.
.br
<utilization>    The resource utilization debited for the job.
.br
A line "::::::::" marks the begin of a new schedule interval.
.br
Please note this file is not truncated. Make sure the monitoring
is switched off in case you have no automated procedure setup that
truncates the schedule file.
.\"
.IP "\fIPE_RANGE_ALG\fP"
This parameter sets the algorithm for the pe range computation. The default is "bin", which
means that the scheduler will use a binary search to select the best one. It should not be necessary to
change it to a different setting in normal operation. If a custom setting is needed, the
following values are available:
.br
auto       : the scheduler selects the best algorithm
.br
least      : starts the resource matching with the lowest slot amount first
.br
bin        : starts the resource matching in the middle of the pe slot range
.br
highest    : starts the resource matching with the highest slot amount first
.\"
.IP "\fIPREFER_SOFT_REQUESTS\fP"
If this parameter is set and resource reservation is enabled for a job then the
scheme for resource reservation is different. Without the parameter set the scheduler
reserves the earliest available resources in time even when soft requests for the
job can not be fulfilled. When the parameter is set then resources are preferred 
which have less infringements for soft requests. 
.\"
.IP "\fIPE_SORT_ORDER\fP"
When using wildcard parallel environment selection during submission time, the parallel
environment the scheduler chooses is arbitrary. In order to fill up the parallel environments
in a specific order this parameter allows to change the sorting of matching parallel
environments either to an ascending or descending order. When PE_SORT_ORDER is set to
ASCENDING (or 1) the first PE which is tested for job selection is the one which is in
alpha-numerical order the first one (test1pe before test2pe and test10pe before test2pe,
when submitting with -pe test*). When it is set to DESCENDING (or 2) the PE which is
tested is in alpha-numerical order the last one (testpe2 in the previous example). When
it is set to 0 or NONE then the first matching PE is arbitrary (default), which is a
good choice for balancing PEs and the same than with absence of the parameter.
.\"
.IP "\fICOUNT_CORES_AS_THREADS\fP"
If set to 1 or TRUE the scheduler treats the requested amount of cores of a job
(with -binding parameter) as request for hardware supported threads. On hosts with
SMT (topology string with threads, like SCTTCTT) the amount of requested cores is divided
by the number of threads per core. In case a core is filled only partially the complete
core is requested by the job. Example: When a job requests 3 cores, on a host with
hyper-threading (2 hardware threads per core) the request is transformed to 2 cores
(because 3 threads are needed). On a host without hyper-threading the job requests
3 cores, and on a host with 4 hardware-threads supported per core the job requests 1 core.
.IP "\fIWRITE_SCHEDD_RUNLOG\fP"
If set equal to 1, scheduler will write trace messages of the next scheduling run
to the file \fI<sge_root>/<cell>/common/schedd_runlog\fP when triggered
by \fIqconf -tsm\fP.\"
Writing the schedd_runlog file can have significant impact on scheduler performance.
This feature should only be enabled when the debugging information contained in the file
is actually needed. Default setting is disabled.
.\"
.IP "\fIMAX_SCHEDULING_TIME\fP"
This parameter can be used to specify a maximum time interval (time_specifier, see
.M sge_types 1)
for one scheduling run. If the scheduler has not finished a dispatching
run within this time interval job dispatching is stopped for this one scheduling run.
In the next scheduling run job dispatching again starts with the highest priority job.
Default for this parameter is 0 (do full dispatching from the highest priority job
down to the lowest priority job).
In huge clusters with a high number of pending jobs setting this parameter to reasonable
values (e.g. one minute) can improve cluster utilization and responsiveness of sge_qmaster.
.\"
.IP "\fIMAX_DISPATCHED_JOBS\fP"
This parameter can be used to limit the number of jobs which get scheduled in one
scheduling interval. Can be set to any positive number or 0 (do not limit the number of scheduled jobs).
Default is 0.
Limiting the number of jobs getting scheduled in a single scheduling interval can be useful
to avoid overload on the cluster, especially on file servers due to many jobs starting up
at the same time.
But use this option with care: Setting it to a too low value can lead to bad utilization of the cluster.
.\"
.IP "\fIHIGH_PRIO_DRAINS_CLUSTER\fP"
When this parameter is set to 1 or TRUE the cluster will be drained until the highest priority
job could be scheduled. This can be used as a workaround to avoid starvation of parallel jobs when
resource reservation cannot be applied, e.g. as job runtimes are unknown.
Use this parameter with care and only temporarily: It can lead to very bad utilization of the cluster.
.\"
.PP
.IP "\fIWARN_DISPATCHING_TIME\fP"
When this parameter is set to a threshold in milliseconds the Univa Grid Engine scheduler will print a warning
to the
.M sge_qmaster 8
messages file when dispatching a job takes longer than the given threshold.
If this parameter is enabled and \fIPROFILE\fP is turned on the profiling output will contain
additional information about the longest and shortest job scheduling time.
The default for "\fIWARN_DISPATCHING_TIME\fP" is 0 (switched off).
.\"
.PP
Changing \fBparams\fP will take immediate effect.
The default for \fBparams\fP is none.
.\"
.SS \fBreprioritize_interval\fP
Interval (HH:MM:SS) to reprioritize jobs on the execution hosts based on the
current ticket amount for the running jobs. If the interval is set to
00:00:00 the reprioritization is turned off. The default value is 00:00:00.
The reprioritization tickets are calculated by the scheduler and update events
for running jobs are only sent after the scheduler calculated new values. How often
the schedule should calculate the tickets is defined by the reprioritize_interval.
Because the scheduler is only triggered in a specific interval (scheduler_interval)
this means the reprioritize_interval has only a meaning if set greater than the scheduler_interval.
For example, if the scheduler_interval is 2 minutes and reprioritize_interval is set
to 10 seconds, this means the jobs get re-prioritized every 2 minutes.
.\"
.SS "\fBreport_pjob_tickets\fP"
This parameter allows to tune the system's scheduling run time. It is used
to enable / disable the reporting of pending job tickets to the qmaster.
It does not influence the tickets calculation. The sort order of jobs in qstat
and qmon is only based on the submit time, when the reporting is turned off.
.br
The reporting should be turned off in a system with a very large amount of jobs
by setting this parameter to "false".
.\"
.SS "\fBhalflife_decay_list\fP"
The halflife_decay_list allows to configure different decay rates for the 
"finished_jobs usage types, which is used in the pending job ticket calculation
to account for jobs which have just ended. This allows the user the pending jobs
algorithm to count finished jobs against a user or project for a configurable decayed 
time period. This feature is turned off by default, and the halftime is used instead.
.br
The halflife_decay_list also allows one to configure different decay rates for each usage 
type being tracked (cpu, io, and mem). The list is specified in the following format:
.sp 1
.nf
.RS
.br
<USAGE_TYPE>=<TIME>[:<USAGE_TYPE>=<TIME>[:<USAGE_TYPE>=<TIME>]]
.RE
.fi
.sp 1
.br
<Usage_TYPE> can be one of the following: cpu, io, or mem.
.br
<TIME> can be -1, 0 or a timespan specified in minutes. If <TIME> is -1, only the usage
of currently running jobs is used. 0 means that the usage is not decayed.
.\"
.SS "\fBpolicy_hierarchy\fP"
This parameter sets up a dependency chain of ticket based
policies. Each ticket based policy in the dependency chain is influenced by the
previous policies and influences the following policies. A typical
scenario is to assign precedence for the override policy over the
share-based policy. The override policy determines in such a case how
share-based tickets are assigned among jobs of the same user or project.
Note that all policies contribute to the ticket amount assigned to a
particular job regardless of the policy hierarchy definition. Yet the
tickets calculated in each of the policies can be different depending on
"\fIPOLICY_HIERARCHY\fP".
.sp 1
The "\fIPOLICY_HIERARCHY\fP" parameter can be a up to 3 letter
combination of the first letters of the 3 ticket based policies S(hare-based),
F(unctional) and O(verride). So a value "OFS" means that the
override policy takes precedence over the functional policy, which
finally influences the share-based policy.
Less than 3 letters mean that some of the policies do not influence
other policies and also are not influenced by other policies. So a value of
"FS" means that the functional policy influences the share-based policy and
that there is no interference with the other policies.
.sp 1
The special value "NONE" switches off policy hierarchies.
.\"
.SS "\fBshare_override_tickets\fP"
If set to "true" or "1", override tickets of any override object instance  
are shared equally among all running jobs associated with the object. The pending
jobs will get as many override tickets, as they would have, when they were
running. If set to "false" or "0", each job gets the full value of the override tickets       
associated with the object. The default value is "true".                   
.\"
.SS "\fBshare_functional_shares\fP"
If set to "true" or "1", functional shares of any functional object instance
are shared among all the jobs associated with the object. If set to "false"
or "0", each job associated with a functional object, gets the full        
functional shares of that object. The default value is "true".            
.\"
.SS "\fBmax_functional_jobs_to_schedule\fP"
The maximum number of pending jobs to schedule in the functional policy.   
The default value is 200.                                                  
.\"
.SS "\fBmax_pending_tasks_per_job\fP"
The maximum number of subtasks per pending array job to schedule. This     
parameter exists in order to reduce scheduling overhead. The default value 
is 50.
.\"
.SS "\fBfair_urgency_list\fP"
A list of complex attributes for which fair urgency shall be applied.
.sp 1
Without fair urgency every job requesting a resource having urgency gets the
full urgency assigned.
With fair urgency the first job requesting a resource gets the full urgency,
the second job gets half of the urgency, the third job a third of the urgency ...
.sp 1
This influences the sorting of the pending job list and can be used to get an even distribution of jobs across multiple resources.
.\"
.SS "\fBmax_reservation\fP"
The maximum number of reservations scheduled within a schedule interval. 
When a runnable job can not be started due to a shortage of resources a 
reservation can be scheduled instead. A reservation can cover consumable 
resources with the global host, any execution host and any queue. For 
parallel jobs reservations are done also for slots resource as specified in
.M sge_pe 5 . 
As job runtime the maximum of the time specified with -l h_rt=... or 
-l s_rt=... or -l d_rt=... is assumed. For jobs that have neither of them the default_duration 
is assumed.
Reservations prevent jobs of lower priority as specified in 
.M sge_priority 5
from utilizing the reserved resource quota during the time of reservation.
Jobs of lower priority are allowed to utilize those reserved resources only 
if their prospective job end is before the start of the reservation (backfilling).
Reservation is done only for non-immediate jobs (-now no) that request reservation 
(-R y). If max_reservation is set to "0" no job reservation is done. 
.sp 1
Note, that reservation scheduling can be performance consuming and hence reservation 
scheduling is switched off by default. Since reservation scheduling performance 
consumption is known to grow with the number of pending jobs, the use of -R y option
is recommended only for those jobs actually queuing for bottleneck resources. 
Together with the max_reservation parameter this technique can be used to narrow 
down performance impacts.
.\"
.SS "\fBdefault_duration\fP"
When job reservation is enabled through max_reservation 
.M sched_conf 5 
parameter the default duration is assumed as runtime for jobs that have 
neither -l h_rt=... nor -l s_rt=... nor -l d_rt=... specified. In contrast to a h_rt/s_rt 
time limit the d_rt and the default_duration are not enforced.
.\"
.SS "\fBbackfilling\fP"
When job reservation is enabled through the max_reservation
.M sched_conf 5
parameter jobs fitting before resource reservations can be backfilled.
Backfilling requires a job runtime specification.
If a job does not request a runtime via
the h_rt, s_rt or d_rt attribute the default duration is assumed as runtime.
Using default duration or a badly estimated d_rt runtime can lead to false backfilling decisions,
therefore the \fBbackfilling\fP parameter allows switching off or limiting the scope of backfilling.
It can be set to one the following values:
.\"
.IP \fIOFF\fP
Scheduler will never do backfilling.
.\"
.IP \fIH_RT\fP
Only jobs requesting a runtime via the h_rt limit can be backfilled.
.\"
.IP \fION\fP
Backfilling is enabled for all jobs types (default).
.\"
.\"
.SS "\fBprioritize_preemptees\fP"
When preemptive scheduling is enabled and when this parameter is set to \fBTRUE\fP then the scheduler
will create a reservation for preemptees before the regular scheduling run is done. This ensures that
preemptees get started again at least when the preemptor finishes, unless resources required by the 
preemptee are still held by jobs which got backfilled. \fBprioritize_preemptees\fP in combination with 
disabling of backfilling (by setting \fBbackfilling\fP to \fBOFF\fP) provides a guarantee that preemptees
get restarted at least when the preemptor finishes, at the expense of lower cluster utilization.
Default for this parameter is \fBFALSE\fP.
.\"
.\"
.SS "\fBpreemptees_keep_resources\fP"
When this parameter is set to \fBTRUE\fP then jobs that get preempted will only be enforced to free
those resources that will be consumed by the job (preemptor) that causes the preemption.
This prevents resources of a preemptee from getting consumed by other jobs. \fBpreemptees_keep_resources\fP 
and \fBprioritize_preemptees\fP in combination provides a guarantee that preemptees get restarted at latest 
when the preemptor finishes, at the expense of a waste of resources and a bad cluster utilization.
One exception from this are software licenses managed through Univa License Orchestrator. Those resources
cannot be held by a preemptee because the preemptee process will be suspended and the underlying license 
manager might assume the license to be free anyways. Default for this parameter is \fBFALSE\fP.
.\"
.\"
.SS "\fBmax_preemptees\fP"
Defines the maximum number of preemptees in the cluster. As preempted jobs might hold some resources 
(memory) and through the \fBpreemptees_keep_resources\fP parameter might even hold most of their resources 
a high number of preemptees can significantly impact cluster operation. Limiting the number of preemptees 
will limit the amount of held but unused resources. Default for this parameter is 0.
.\"
.\"
.SS "\fBpreemption_distance\fP"
A preemption will only be triggered if the resource reservation that could be done for a job is farther 
in the future than the given time interval (hh:mm:ss). Reservation can be disabled by 
setting the value to \fB00:00:00\fP. Reservation will also be omitted if preemption of jobs is forced manually 
using 'qmod -f -p ... S|N|P'. Default for this parameter is \fB00:15:00\fP.
.\"
.\"
.SS "\fBpreemption_priority_adjustments\fP"
This parameter allows to automatically adjust the POSIX priority of running jobs depending on their type 
or state. This will influence the normalized and weighted priority (prio as show by qstat) before 
running jobs are considered as preemption candidates. As default this parameter is set to \fBNONE\fP but 
it is allowed to set it to a list of name/value-pairs. The name of such an entry defines a possible type, 
state or other characteristic of a running job and the value defines the new POSIX priority or a relative 
POSIX priority adjustment. 
.sp 1
Name/value-pairs have to be separated by comma (','). Delimiting character for name and value is the 
equal-character ('='). Priority values that would leave the allowed POSIX priority range will be 
automatically set to the smallest or biggest priority value depending on the limit that is exceeded. 
.sp 1
Adjustment value might be in range from -1023 to 1024. Relative values start with the letter 'd' (for delta) 
and have to be in range -2047 to 2047 (e.g 'd-100' to decrease the POSIX priority by 100). 
.sp 1
Please note that currently the list may only contain one name/value pair but this may change with each
patch release of UGE. If the list contains multiple entries then all of them are considered from left
to right (first to last entry).
.sp 1
.IP "\fIALREADY_PREEMPTED\fP"
Adjusts the priority of jobs that have been restarted after preemption. Prevent that jobs that have been 
restarted after preemption get immediately preempted again by a higher priority job.
.\"
.\"
.SH FILES
.nf
.ta \w'<sge_root>/'u
\fI<sge_root>/<cell>/common/sched_configuration\fP
	scheduler thread configuration
.fi
.\"
.\"
.SH "SEE ALSO"
.M sge_intro 1 ,
.M qalter 1 ,
.M qconf 1 ,
.M qstat 1 ,
.M qsub 1 ,
.M complex 5 ,
.M queue_conf 5 ,
.M sge_execd 8 ,
.M sge_qmaster 8 ,
.I Univa Grid Engine Installation and Administration Guide
.\"
.SH "COPYRIGHT"
See
.M sge_intro 1
for a full statement of rights and permissions.
